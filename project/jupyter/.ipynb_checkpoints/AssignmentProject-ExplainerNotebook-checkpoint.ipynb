{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our idea is to take the Global Terrorism Database and perform a comprehensive analysis on it in order to get insight into the terrorism and create tools which let the user to explore the data interactively. The dataset is semistructured so it requires a lot of cleaning and preprocessing tasks in order to make it accessible for the entire project, hence we use some of the key techniques introduced in the course. First of all, we format and clean the data. Afterwards, we compress it to the client-side friendly form to require less space and make the UI more responsive. We then create many kinds of charts using D3.js, each having its own function and key objectives. \n",
    "\n",
    "<span style=\"color: orange\"><b>Important: We decided to distribute code in separate notebooks.</b></span>\n",
    "\n",
    "<span style=\"color: orange\"><b>To find the code and corresponding documentation on each analysis step just jump to the linked notebooks below.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Part II - Data Preprocessing](http://nbviewer.jupyter.org/github/polakowo/socialdata2017/blob/master/project/jupyter/AssignmentProject-DataPreprocessing.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we decompress, clean and format the data (so perform the initial preprocessing of the data) to a shape which is best for use in further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Part II - Stats](http://nbviewer.jupyter.org/github/polakowo/socialdata2017/blob/master/project/jupyter/AssignmentProject-Stats.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we do some basic analysis using `Plotly`, which is beyond the scope of this project but we liked the idea to show something in the static notebook in case something goes wrong online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to dive a bit more into visualizations and limit used machine learning methods to 2, both of which must be interactive enough to embed them into charts. Hence, we use k-nearest neighbors and k-means, both of which just make fun to play with.\n",
    "\n",
    "We didn't go further to multiple/logistic regression, because we encountered some constraints using them with our data. The most of the predicting features we can feed to the classifier are categorical, meaning that each algorithm would have preferred the feature with the most classes, leading to a clear bias. We also have seen some of the examples on Kaggle, where no model performance was higher than middle score. \n",
    "\n",
    "<span style=\"color: orange;\"><b>More on KNN and K-Means you find in the section Visualizations.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### [Part III - Data for Visualization](http://nbviewer.jupyter.org/github/polakowo/socialdata2017/blob/master/project/jupyter/AssignmentProject-VizData.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you find the code for aggregation of data we use in all of our charts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of using histogram to highlight the temporal development of terrorism is straightforward. Each bar corresponds to a year. The length of a bar is linked to the killed/wounded/attacks in that year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scatterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare different weapon, attack or target types, we would like to use as many dimensions as possible. For this, the scatterplot is almost perfect, as it enables encoding of 4 different metrics (X, Y, size, color)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Choropleth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplot above has one big issue: we can display up to ~30 circles before we run out of space. But what if we'd love to compare countries? Even on a rectangular map with Mercador projection we need some kind of zoom. To tackle the problem we decided on another, more difficult, but also interesting solution: map countries on a virtual globe and let the user rotate it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization of K-Means should let the user customize the number of groups, as well as to display the centroids and the data points they belong to. Here we arrive at the next problem: how do we display all 150,000 terrorist attacks on a small map?\n",
    "\n",
    "**Solution: We use hexbins!**\n",
    "\n",
    "Each hexagonal bin is a shape which aggregates attacks occurred in a area of a specific radius. This way we avoid plotting thousands of data points. Instead, we just plot hexbins and display further information on hover. Brilliant, isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first idea we had on visualization was to create a grid of already classified points with `iPython`, and import it into `d3.js`. But we encountered many problems:\n",
    "* We would have to calculate and store grids for each `k` and `Type`, which is space expensive\n",
    "* We would have struggled with displaying neighbors\n",
    "* But also lack of interactivity would make the things looking a bit static.\n",
    "\n",
    "We ended up with the calculation of kNN on fly. This way, the visualization allows to classify the point user explicitly selects on the map. It also shows the related neighbors and their types without flooding the map with data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of data from the Global Terrorism Database was very challenging, as the provided data was encoded by rules specified in the (very, very long) codebook. That is, we had to decode every single column we were interested in, clean it from bad values like negatives, zeros, or nans. Decide how we proceed with unknown values. And reformat the data for further use (we used `pandas.DataFrame` for that, pretty handsome). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we've done right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing we are really proud of is our super-flexible client-side infrastructure: we have almost no hard-coded data structures, variables or constants. The JavaScript functions we provide take care of recognizing what the corresponding csv-file contains and populates the dropdowns and charts automatically! By changing csv-files with iPython you're good with refreshing the page to see the changed data to be visualized, (maybe almost) without rewriting `js`-functions. Every piece of code was aimed at flexibility and customization. You can put almost any kind of information into those charts just by knowing how they handle the data (please read the comments). \n",
    "\n",
    "This also applies to the project structure, which (we hope) complies with many principles of a good design. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we haven't done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't train a set of classifiers to predict things - we looked at Kaggle competitions and did notice an almost zero classifier performance for the most of attributes, hence we ignored this step. We decided to dig into the visualizations and UX a bit deeper. We hope you will have a lot of fun."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
